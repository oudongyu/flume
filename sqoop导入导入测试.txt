我需要将mysql的一张表的数据放到hdfs上边。
sqoop import --connect jdbc:mysql://192.168.8.101:3306/sqoop_tmp --username root --password 123456 --table student02 -m 1


指定导入hdfs的路径
sqoop import --connect jdbc:mysql://192.168.8.101:3306/sqoop_tmp --username root --password 123456 --table student02 -m 1 --target-dir /sqoop_tmp/student01


导入id大于4的数据到hdfs
sqoop import --connect jdbc:mysql://192.168.8.101:3306/sqoop_tmp --username root  --password 123456 --table student02 -m 1 --append --check-column 'id' --incremental append --last-value 4

sqoop  query，通过sql语句将指定数据写入hdfs(需要指定jdbc的ip和端口号。以及用户名和密码，-m指定map task的数量，--target-dir指定hdfs的路径，--fields-terminated-by元素之间的分隔符，--split-by)
sqoop import --connect jdbc:mysql://192.168.8.101:3306/sqoop_tmp --username root  --password 123456 --query "select id,name,sex,date,content from student02 where \$CONDITIONS and id=2" -m 1 --target-dir /sqoop_tmp/student02  --fields-terminated-by "," --split-by ","

sqoop import --connect jdbc:mysql://192.168.8.101:3306/sqoop_tmp --username root  --password 123456 --query "select id,name,sex,date,content from student02 where \$CONDITIONS" -m 1 --target-dir /sqoop_tmp/student02  --fields-terminated-by "," --split-by ","


将数据入到hive表(。)
sqoop import --connect jdbc:mysql://192.168.8.101:3306/sqoop_tmp --username root  --password 123456 --table student02 --hive-import -m 1 --fields-terminated-by " "

将hdfs指定路径的数据写入mysql表(注意：1.mysql表要存在;2.注意分隔符，默认是逗号分隔)
sqoop export --connect "jdbc:mysql://192.168.8.101:3306/sqoop_tmp?useUnicode=true&characterEncoding=utf-8" --username root --password 123456 --table dw_top_hot --fields-terminated-by "\001" --export-dir /user/hive/warehouse/dw_top_hot/000000_0
	

sqoop export --connect "jdbc:mysql://192.168.8.101:3306/sqoop_tmp?useUnicode=true&characterEncoding=utf-8" --username root --password 123456 --table dw_top_hot --fields-terminated-by "\001" --export-dir /user/hive/warehouse/movierating














